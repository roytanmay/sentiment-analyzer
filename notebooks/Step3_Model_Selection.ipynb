{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re, string, unicodedata\n",
    "from string import punctuation\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'G:\\ML projects\\IMDB-Dataset.csv', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning And Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his', 'again', \"it's\", 'yours', 'through', 'over', 's', \"haven't\", 'you', 'most', 'then', 'so', 'while', 'how', 'hasn', 'your', \"isn't\", 'at', 'from', 'my', \"wasn't\", 'me', 'in', 'don', 'been', 'to', 'isn', 'director', 'between', 'a', 'him', 'up', 'her', 'film', 'during', 'haven', 'its', 'myself', \"don't\", 'other', 'yourselves', 'movie', 'mustn', 'doesn', 'herself', 'was', 'same', 'very', 'just', \"won't\", \"you'll\", 've', 'and', 'why', 'own', 'is', 'our', 'about', \"she's\", 'hadn', 'into', 'weren', 'or', 'their', 'm', 'whom', \"hadn't\", 'down', 'could', \"couldn't\", 'before', \"needn't\", 'couldn', 'wasn', 'all', 'ain', 'against', 'shouldn', 'the', 'out', 'as', \"that'll\", 'until', 'might', 'being', \"weren't\", 'if', 'only', 'than', 'having', 'were', 'are', 'which', 'has', 'he', 'had', 'aren', 'actress', 'each', 'them', 'of', 'shan', 'what', 'under', 'it', 't', 'below', 'ma', 'theirs', 'for', 'i', 'needn', 'would', \"aren't\", 'character', 'by', 'have', 'shall', 'this', 'd', 'who', \"wouldn't\", 'nor', 'once', 'should', 'am', 'off', 'can', 'll', \"doesn't\", 'above', \"mustn't\", 'did', 'such', \"didn't\", \"you'd\", 'himself', 'hers', 'after', 'they', 'those', 'further', 'these', 'will', 'won', \"shan't\", 'itself', 'y', 'both', \"shouldn't\", 'some', 'but', 'yourself', 'themselves', 'didn', 'actor', 'be', 'ourselves', 'do', 'she', 're', 'mightn', 'here', 'does', \"hasn't\", \"mightn't\", \"you've\", 'with', \"you're\", 'any', 'wouldn', 'because', 'there', 'doing', 'when', \"should've\", 'scene', 'on', 'where', 'few', 'too', 'an', 'more', 'now', 'ours', 'o', 'that', 'we'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "new_stopwords = [\"would\", \"shall\", \"could\", \"might\", \"film\", \"movie\", \"director\", \"scene\", \"character\", \"actor\", \"actress\"]\n",
    "stop_words.extend(new_stopwords)\n",
    "\n",
    "negations_and_sentiment_words = [\"not\", \"no\", \"never\", \"n't\", \"none\", \"good\", \"bad\", \"love\", \"hate\"]\n",
    "\n",
    "for word in negations_and_sentiment_words:\n",
    "    if word in stop_words:\n",
    "        stop_words.remove(word)\n",
    "\n",
    "stop_words = set(stop_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------Data Cleaning and Preprocessing pipeline----------------------------------'''\n",
    "\n",
    "#Removing special character\n",
    "def remove_special_character(content):\n",
    "    # return re.sub(r'\\W+',' ', content )\n",
    "    return re.sub(r'\\[[^&@#!]]*\\]', '', content)\n",
    "\n",
    "# Removing URL's\n",
    "def remove_url(content):\n",
    "    return re.sub(r'http\\S+', '', content)\n",
    "\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(content):\n",
    "    clean_data = []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)\n",
    "\n",
    "# Expansion of english contractions\n",
    "def contraction_expansion(content):\n",
    "    content = re.sub(r\"won\\'t\", \"would not\", content)\n",
    "    content = re.sub(r\"can\\'t\", \"can not\", content)\n",
    "    content = re.sub(r\"don\\'t\", \"do not\", content)\n",
    "    content = re.sub(r\"shouldn\\'t\", \"should not\", content)\n",
    "    content = re.sub(r\"needn\\'t\", \"need not\", content)\n",
    "    content = re.sub(r\"hasn\\'t\", \"has not\", content)\n",
    "    content = re.sub(r\"haven\\'t\", \"have not\", content)\n",
    "    content = re.sub(r\"weren\\'t\", \"were not\", content)\n",
    "    content = re.sub(r\"mightn\\'t\", \"might not\", content)\n",
    "    content = re.sub(r\"didn\\'t\", \"did not\", content)\n",
    "    content = re.sub(r\"n\\'t\", \" not\", content)\n",
    "    '''content = re.sub(r\"\\'re\", \" are\", content)\n",
    "    content = re.sub(r\"\\'s\", \" is\", content)\n",
    "    content = re.sub(r\"\\'d\", \" would\", content)\n",
    "    content = re.sub(r\"\\'ll\", \" will\", content)\n",
    "    content = re.sub(r\"\\'t\", \" not\", content)\n",
    "    content = re.sub(r\"\\'ve\", \" have\", content)\n",
    "    content = re.sub(r\"\\'m\", \" am\", content)'''\n",
    "    return content\n",
    "\n",
    "#Data preprocessing\n",
    "def data_cleaning(content):\n",
    "    content = contraction_expansion(content)\n",
    "    content = remove_special_character(content)\n",
    "    content = remove_url(content)\n",
    "    \n",
    "    content = remove_stopwords(content)    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Movies</th>\n",
       "      <th>Resenhas</th>\n",
       "      <th>Reviews_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>*Disclaimer: I only watched this movie as a conditional agreement. And I see films for free. I wouldn't be caught dead giving my hard earned money to these idiots.Well, to explain the depth of this 'film', I could write my shortest review, ever. Don't see this movie. It is by far the stupidest, lamest, most lazy, and unbelievably UNFUNNY movie I have ever seen. It is a total disaster. But since my hatred for this movie, and the others like it, extends far beyond one viewing, I think I'll go on for a bit.I don't know any of the people in the movie besides Carmen Electra, Vanessa Minnillo, and Kim Kardashian, but it doesn't matter. They're all horrible, though I think that was the point. The editing is flat out horrible, and possibly blatant continuity errors make this crapfast even crappier than I thought it would be. Now I know that these films are not supposed to be serious at all, but come on, it's film-making 101 that if someone gets a minor facial cut, it should be there in the...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>* IsenÃ§Ã£o de responsabilidade: eu sÃ³ assisti esse filme como um acordo condicional. E eu vejo filmes de graÃ§a. Eu nÃ£o seria pego morto dando meu dinheiro suado a esses idiotas. Bem, para explicar a profundidade desse 'filme', eu poderia escrever minha crÃ­tica mais curta de todos os tempos. NÃ£o vÃª este filme. Ã de longe o filme mais estÃºpido, lamenta, preguiÃ§oso e inacreditavelmente UNFUNNY que eu jÃ¡ vi. Ã um desastre total. Mas como o meu Ã³dio por este filme e por outros, se estende muito alÃ©m de uma exibiÃ§Ã£o, acho que vou continuar um pouco. NÃ£o conheÃ§o nenhuma das pessoas do filme alÃ©m de Carmen Electra, Vanessa Minnillo, e Kim Kardashian, mas isso nÃ£o importa. Eles sÃ£o todos horrÃ­veis, embora eu ache que esse seja o ponto. A ediÃ§Ã£o Ã© horrÃ­vel e, possivelmente, erros de continuidade flagrantes tornam essa porcaria ainda mais horrÃ­vel do que eu pensava. Agora eu sei que esses filmes nÃ£o devem ser sÃ©rios, mas vamos lÃ¡, Ã© o cinema 101 que se alguÃ©m f...</td>\n",
       "      <td>watched conditional see films not caught dead giving hard earned money explain depth write shortest not see far unbelievably unfunny ever total since hatred others like extends far beyond one think go not know people besides carmen vanessa kim not though think editing flat possibly blatant continuity errors make crapfast even crappier thought know films not supposed serious come someone gets minor facial next someone gets cut blood least cut since narnia films away give disaster pass thoughtless mindless physical gags obviously take popular movies last year late including best picture know saddest thing stupid movies not care much money many cameos sorry ass excuses films taking away jobs directors truly deserve thought better taste ashamed making kind jason friedberg aaron burn guys contributing decline western cause downfall western</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I am writing this in hopes that this gets put over the previous review of this \"film\". How anyone can find this slop entertaining is completely beyond me. First of all a spoof film entitled \"Disaster Movie\", should indeed be a spoof on disaster films. Now I have seen 1 (yes count them, 1) disaster film being spoofed, that being \"Twister\". How does Juno, Iron Man, Batman, The Hulk, Alvin and the Chipmunks, Amy Winehouse, or Hancock register as Disaster films? Selzterwater and Failburg once again have shown that they lack any sort of writing skill and humor. Having unfortunately been tortured with Date Movie and Epic Movie I know exactly what to expect from these two...no plot, no jokes just bad references and cheaply remade scenes from other films. Someone should have informed them that satire is more than just copy and paste from one film to another, though I shouldn't say that because some of these actually just seem to be taken from trailers.There is nothing clever or witty or re...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Estou escrevendo isso na esperanÃ§a de que isso seja colocado sobre a revisÃ£o anterior deste \"filme\". Como alguÃ©m pode achar divertido esse desleixo estÃ¡ completamente alÃ©m de mim. Antes de mais nada, um filme de parÃ³dia intitulado \"Filme de desastre\" deveria ser, de fato, uma parÃ³dia de filmes de desastre. Agora eu jÃ¡ vi 1 (sim, conte-os, 1) filme de desastre sendo falsificado, sendo \"Twister\". Como Juno, Homem de Ferro, Batman, O Hulk, Alvin e os Esquilos, Amy Winehouse ou Hancock se registram como filmes de Desastre? Selzterwater e Failburg mostraram mais uma vez que nÃ£o possuem nenhum tipo de habilidade e humor de escrita. Infelizmente, tendo sido torturado com Date Movie e Epic Movie, sei exatamente o que esperar desses dois ... nenhum enredo, nenhuma piada, apenas mÃ¡s referÃªncias e cenas refeitas de outros filmes. AlguÃ©m deveria ter informado a eles que a sÃ¡tira Ã© mais do que apenas copiar e colar de um filme para outro, embora eu nÃ£o deva dizer isso porque algu...</td>\n",
       "      <td>writing hopes gets put previous review anyone find slop entertaining completely beyond first spoof entitled indeed spoof disaster seen count disaster iron alvin amy hancock register disaster selzterwater failburg shown lack sort writing skill unfortunately tortured date epic know exactly expect no jokes bad references cheaply remade scenes someone informed satire copy paste one though not say actually seem taken nothing clever witty remotely smart way two not believe people still pay see insult though enjoy films doubt smart enough realize unfortunately not number low enough includes rate deserves top worst films right date epic mean meet rather forced hour hands marathon watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Really, I could write a scathing review of this turd sandwich, but instead, I'm just going to be making a few observations and points I've deduced.There's just no point in watching these movies anymore. Does any reader out there remember Scary Movie? Remember how it was original with a few comedic elements to it? There was slapstick, some funny lines, it was a pretty forgettable comedy, but it was worth the price of admission. Well, That was the last time this premise was funny. STOP MAKING THESE MOVIES. PLEASE.I could call for a boycott of these pieces of monkey sh*t, but we all know there's going to be a line up of pre pubescent annoying little buggers, spouting crappy one liners like, \"THIS IS SPARTA!\" and, \"IM RICK JAMES BITCH\" so these movies will continue to make some form of monetary gain, considering the production value of this movie looks like it cost about 10 cents to make.Don't see this movie. Don't spend any money on it. Go home, rent Airplane, laugh your ass off, and ...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Realmente, eu poderia escrever uma crÃ­tica contundente sobre esse sanduÃ­che de cocÃ´, mas, em vez disso, vou fazer algumas observaÃ§Ãµes e pontos que deduzi. NÃ£o hÃ¡ mais sentido assistir a esses filmes. Algum leitor por aÃ­ se lembra do filme de terror? Lembra como era original, com alguns elementos cÃ´micos? Havia palhaÃ§ada, algumas frases engraÃ§adas, era uma comÃ©dia bastante esquecÃ­vel, mas valia o preÃ§o da entrada. Bem, essa foi a Ãºltima vez que essa premissa foi engraÃ§ada. PARE DE FAZER ESTES FILMES. POR FAVOR, eu poderia pedir um boicote a esses pedaÃ§os de macaco, mas todos sabemos que haverÃ¡ uma fila de buggers irritantes e prÃ©-pubescentes, jorrando uns forros ruins como: \"ISTO Ã SPARTA!\" e \"IM RICK JAMES BITCH\", para que esses filmes continuem gerando algum ganho monetÃ¡rio, considerando que o valor de produÃ§Ã£o deste filme parece custar cerca de 10 centavos de dÃ³lar. NÃ£o gaste dinheiro com isso. VÃ¡ para casa, alugue a Airplane, ria e julgue silenciosament...</td>\n",
       "      <td>write scathing review turd going making observations points no point watching movies reader remember scary remember original comedic elements funny pretty forgettable worth price last time premise stop making call boycott pieces monkey know going line pre pubescent annoying little spouting crappy one liners rick james movies continue make form monetary considering production value looks like cost cents not see not spend money go rent laugh ass silently judge people talking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>If you saw the other previous spoof movies by these two horrible gentlemen, then you should know that this already will be bad. I'll tell you the truth, if you want to watch it as a brainless person (ironically meant for the stereotypical teenagers, which I am not) then you will laugh at it a bit. But if you judge it, even a little, the movie automatically fails. Why? Never ask that when it comes to these two men.Remember the good old Hollywood days whenever making a movie was about showing people a type of art, and also a story that kept you on the edge of your seat? Well whenever word hit that making films earned you loads of cash, then all these greedy people came in the picture and its quite pathetic. These two are no exception. We still have movie artists (most notably the genius that is Christopher Nolan). But these two guys just...well I've been writing so big words, let me put it in simple terms for these guys...These guys suck, they are not artists, but instead money cravi...</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Se vocÃª viu os outros filmes falsificados anteriores por esses dois senhores horrÃ­veis, deve saber que isso jÃ¡ serÃ¡ ruim. Vou lhe dizer a verdade, se vocÃª quiser vÃª-lo como uma pessoa sem cÃ©rebro (ironicamente para os adolescentes estereotipados, o que eu nÃ£o sou), entÃ£o vocÃª rirÃ¡ um pouco. Mas se vocÃª julgar, mesmo que um pouco, o filme falha automaticamente. Por quÃª? Nunca pergunte isso quando se trata desses dois homens. Lembre-se dos bons e velhos tempos de Hollywood, sempre que fazer um filme era mostrar Ã s pessoas um tipo de arte e tambÃ©m uma histÃ³ria que o mantinha na ponta do seu assento? Bem, sempre que a notÃ­cia de que fazer filmes ganhava muito dinheiro, entÃ£o todas essas pessoas gananciosas apareciam na imagem e Ã© bastante patÃ©tico. Esses dois nÃ£o sÃ£o exceÃ§Ã£o. Ainda temos artistas de filmes (principalmente o gÃªnio Christopher Nolan). Mas esses dois caras simplesmente ... bem, eu tenho escrito palavras tÃ£o grandes, deixe-me colocar em termos sim...</td>\n",
       "      <td>saw previous spoof movies two horrible know already tell want watch brainless person meant stereotypical laugh judge even automatically never ask comes two good old hollywood days whenever making showing people type also story kept edge well whenever word hit making films earned loads greedy people came picture quite two no still artists notably genius christopher two guys writing big let put simple terms guys not instead money craving latest proves even fails easily mind mean nothing funny people usually put best stuff idiots knew going made bet not good idea write reviews paper tell everyone whats good whats flipped review well warning not even called nothing artistic references made throughout pretty much like hannah montana juno gig actually close spoofing failed referencing instead joking twisting random wrestling not know high respect high respect know not something not add story nudity not really needed closest still gotten idea saw bare hate girl says guys perverts brainles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>This movie I saw a day early for free and I still feel like I got ripped off. It is totally brain dead. Burping, kicking in the groin and boobs all over the place. Lame. What is wrong with society, that films like this even get made? The parodies were all horrendous, and un-funny. The plot was lackluster at best and the acting was shallow, transparent and really quite unnecessary.Anyone see \"Idiocracy\"? Remember the movie that won all the academy awards in the future? Well this is that movie. I have not seen a more rancid crappy film. \"Date Movie\" was okay, The Scary movies at least had decent plots, but this, this makes \"spoofs\" (if I can be so nice to call it that) for this year 0 for 3, with \"Meet the Spartans\" and \"Superhero Movie\" all falling flat.Well I've wasted even more of my life typing about this sack of cow dung. So all in all, don't see this movie, unless of course your IQ is below 80.Thanks, R</td>\n",
       "      <td>Disaster Movie</td>\n",
       "      <td>Este filme eu vi um dia cedo de graÃ§a e ainda sinto que fui enganado. Ã totalmente morte cerebral. Arrotando, chutando a virilha e os peitos por todo o lugar. Coxo. O que hÃ¡ de errado com a sociedade, que filmes como esse sÃ£o feitos? As parÃ³dias eram todas horrendas e pouco engraÃ§adas. O enredo foi sem brilho, na melhor das hipÃ³teses, e a atuaÃ§Ã£o foi superficial, transparente e realmente bastante desnecessÃ¡ria. AlguÃ©m vÃª \"Idiocracia\"? Lembra do filme que ganhou todos os prÃªmios da academia no futuro? Bem, este Ã© esse filme. Eu nÃ£o vi um filme de baixa qualidade mais ranÃ§oso. \"Date Movie\" foi bom, The Scary Movies pelo menos teve enredos decentes, mas isso faz \"spoofs\" (se Ã© que posso dizer assim) para este ano 0 para 3, com \"Meet the Spartans\" e \"Filme de super-herÃ³is\" todos caindo. Bem, eu perdi ainda mais da minha vida digitando sobre esse saco de esterco de vaca. EntÃ£o, apesar de tudo, nÃ£o assista a este filme, a menos que o seu QI seja inferior a 80.</td>\n",
       "      <td>saw day early free still feel like got ripped totally brain kicking groin boobs wrong films like even get parodies plot lackluster best acting transparent really quite see remember academy awards well not seen rancid crappy scary movies least decent makes nice call year falling wasted even life typing sack cow not see unless course iq r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings  \\\n",
       "0      1.0   \n",
       "1      1.0   \n",
       "2      1.0   \n",
       "3      1.0   \n",
       "4      1.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Reviews  \\\n",
       "0  *Disclaimer: I only watched this movie as a conditional agreement. And I see films for free. I wouldn't be caught dead giving my hard earned money to these idiots.Well, to explain the depth of this 'film', I could write my shortest review, ever. Don't see this movie. It is by far the stupidest, lamest, most lazy, and unbelievably UNFUNNY movie I have ever seen. It is a total disaster. But since my hatred for this movie, and the others like it, extends far beyond one viewing, I think I'll go on for a bit.I don't know any of the people in the movie besides Carmen Electra, Vanessa Minnillo, and Kim Kardashian, but it doesn't matter. They're all horrible, though I think that was the point. The editing is flat out horrible, and possibly blatant continuity errors make this crapfast even crappier than I thought it would be. Now I know that these films are not supposed to be serious at all, but come on, it's film-making 101 that if someone gets a minor facial cut, it should be there in the...   \n",
       "1  I am writing this in hopes that this gets put over the previous review of this \"film\". How anyone can find this slop entertaining is completely beyond me. First of all a spoof film entitled \"Disaster Movie\", should indeed be a spoof on disaster films. Now I have seen 1 (yes count them, 1) disaster film being spoofed, that being \"Twister\". How does Juno, Iron Man, Batman, The Hulk, Alvin and the Chipmunks, Amy Winehouse, or Hancock register as Disaster films? Selzterwater and Failburg once again have shown that they lack any sort of writing skill and humor. Having unfortunately been tortured with Date Movie and Epic Movie I know exactly what to expect from these two...no plot, no jokes just bad references and cheaply remade scenes from other films. Someone should have informed them that satire is more than just copy and paste from one film to another, though I shouldn't say that because some of these actually just seem to be taken from trailers.There is nothing clever or witty or re...   \n",
       "2  Really, I could write a scathing review of this turd sandwich, but instead, I'm just going to be making a few observations and points I've deduced.There's just no point in watching these movies anymore. Does any reader out there remember Scary Movie? Remember how it was original with a few comedic elements to it? There was slapstick, some funny lines, it was a pretty forgettable comedy, but it was worth the price of admission. Well, That was the last time this premise was funny. STOP MAKING THESE MOVIES. PLEASE.I could call for a boycott of these pieces of monkey sh*t, but we all know there's going to be a line up of pre pubescent annoying little buggers, spouting crappy one liners like, \"THIS IS SPARTA!\" and, \"IM RICK JAMES BITCH\" so these movies will continue to make some form of monetary gain, considering the production value of this movie looks like it cost about 10 cents to make.Don't see this movie. Don't spend any money on it. Go home, rent Airplane, laugh your ass off, and ...   \n",
       "3  If you saw the other previous spoof movies by these two horrible gentlemen, then you should know that this already will be bad. I'll tell you the truth, if you want to watch it as a brainless person (ironically meant for the stereotypical teenagers, which I am not) then you will laugh at it a bit. But if you judge it, even a little, the movie automatically fails. Why? Never ask that when it comes to these two men.Remember the good old Hollywood days whenever making a movie was about showing people a type of art, and also a story that kept you on the edge of your seat? Well whenever word hit that making films earned you loads of cash, then all these greedy people came in the picture and its quite pathetic. These two are no exception. We still have movie artists (most notably the genius that is Christopher Nolan). But these two guys just...well I've been writing so big words, let me put it in simple terms for these guys...These guys suck, they are not artists, but instead money cravi...   \n",
       "4                                                                                 This movie I saw a day early for free and I still feel like I got ripped off. It is totally brain dead. Burping, kicking in the groin and boobs all over the place. Lame. What is wrong with society, that films like this even get made? The parodies were all horrendous, and un-funny. The plot was lackluster at best and the acting was shallow, transparent and really quite unnecessary.Anyone see \"Idiocracy\"? Remember the movie that won all the academy awards in the future? Well this is that movie. I have not seen a more rancid crappy film. \"Date Movie\" was okay, The Scary movies at least had decent plots, but this, this makes \"spoofs\" (if I can be so nice to call it that) for this year 0 for 3, with \"Meet the Spartans\" and \"Superhero Movie\" all falling flat.Well I've wasted even more of my life typing about this sack of cow dung. So all in all, don't see this movie, unless of course your IQ is below 80.Thanks, R   \n",
       "\n",
       "           Movies  \\\n",
       "0  Disaster Movie   \n",
       "1  Disaster Movie   \n",
       "2  Disaster Movie   \n",
       "3  Disaster Movie   \n",
       "4  Disaster Movie   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Resenhas  \\\n",
       "0  * IsenÃ§Ã£o de responsabilidade: eu sÃ³ assisti esse filme como um acordo condicional. E eu vejo filmes de graÃ§a. Eu nÃ£o seria pego morto dando meu dinheiro suado a esses idiotas. Bem, para explicar a profundidade desse 'filme', eu poderia escrever minha crÃ­tica mais curta de todos os tempos. NÃ£o vÃª este filme. Ã de longe o filme mais estÃºpido, lamenta, preguiÃ§oso e inacreditavelmente UNFUNNY que eu jÃ¡ vi. Ã um desastre total. Mas como o meu Ã³dio por este filme e por outros, se estende muito alÃ©m de uma exibiÃ§Ã£o, acho que vou continuar um pouco. NÃ£o conheÃ§o nenhuma das pessoas do filme alÃ©m de Carmen Electra, Vanessa Minnillo, e Kim Kardashian, mas isso nÃ£o importa. Eles sÃ£o todos horrÃ­veis, embora eu ache que esse seja o ponto. A ediÃ§Ã£o Ã© horrÃ­vel e, possivelmente, erros de continuidade flagrantes tornam essa porcaria ainda mais horrÃ­vel do que eu pensava. Agora eu sei que esses filmes nÃ£o devem ser sÃ©rios, mas vamos lÃ¡, Ã© o cinema 101 que se alguÃ©m f...   \n",
       "1  Estou escrevendo isso na esperanÃ§a de que isso seja colocado sobre a revisÃ£o anterior deste \"filme\". Como alguÃ©m pode achar divertido esse desleixo estÃ¡ completamente alÃ©m de mim. Antes de mais nada, um filme de parÃ³dia intitulado \"Filme de desastre\" deveria ser, de fato, uma parÃ³dia de filmes de desastre. Agora eu jÃ¡ vi 1 (sim, conte-os, 1) filme de desastre sendo falsificado, sendo \"Twister\". Como Juno, Homem de Ferro, Batman, O Hulk, Alvin e os Esquilos, Amy Winehouse ou Hancock se registram como filmes de Desastre? Selzterwater e Failburg mostraram mais uma vez que nÃ£o possuem nenhum tipo de habilidade e humor de escrita. Infelizmente, tendo sido torturado com Date Movie e Epic Movie, sei exatamente o que esperar desses dois ... nenhum enredo, nenhuma piada, apenas mÃ¡s referÃªncias e cenas refeitas de outros filmes. AlguÃ©m deveria ter informado a eles que a sÃ¡tira Ã© mais do que apenas copiar e colar de um filme para outro, embora eu nÃ£o deva dizer isso porque algu...   \n",
       "2  Realmente, eu poderia escrever uma crÃ­tica contundente sobre esse sanduÃ­che de cocÃ´, mas, em vez disso, vou fazer algumas observaÃ§Ãµes e pontos que deduzi. NÃ£o hÃ¡ mais sentido assistir a esses filmes. Algum leitor por aÃ­ se lembra do filme de terror? Lembra como era original, com alguns elementos cÃ´micos? Havia palhaÃ§ada, algumas frases engraÃ§adas, era uma comÃ©dia bastante esquecÃ­vel, mas valia o preÃ§o da entrada. Bem, essa foi a Ãºltima vez que essa premissa foi engraÃ§ada. PARE DE FAZER ESTES FILMES. POR FAVOR, eu poderia pedir um boicote a esses pedaÃ§os de macaco, mas todos sabemos que haverÃ¡ uma fila de buggers irritantes e prÃ©-pubescentes, jorrando uns forros ruins como: \"ISTO Ã SPARTA!\" e \"IM RICK JAMES BITCH\", para que esses filmes continuem gerando algum ganho monetÃ¡rio, considerando que o valor de produÃ§Ã£o deste filme parece custar cerca de 10 centavos de dÃ³lar. NÃ£o gaste dinheiro com isso. VÃ¡ para casa, alugue a Airplane, ria e julgue silenciosament...   \n",
       "3  Se vocÃª viu os outros filmes falsificados anteriores por esses dois senhores horrÃ­veis, deve saber que isso jÃ¡ serÃ¡ ruim. Vou lhe dizer a verdade, se vocÃª quiser vÃª-lo como uma pessoa sem cÃ©rebro (ironicamente para os adolescentes estereotipados, o que eu nÃ£o sou), entÃ£o vocÃª rirÃ¡ um pouco. Mas se vocÃª julgar, mesmo que um pouco, o filme falha automaticamente. Por quÃª? Nunca pergunte isso quando se trata desses dois homens. Lembre-se dos bons e velhos tempos de Hollywood, sempre que fazer um filme era mostrar Ã s pessoas um tipo de arte e tambÃ©m uma histÃ³ria que o mantinha na ponta do seu assento? Bem, sempre que a notÃ­cia de que fazer filmes ganhava muito dinheiro, entÃ£o todas essas pessoas gananciosas apareciam na imagem e Ã© bastante patÃ©tico. Esses dois nÃ£o sÃ£o exceÃ§Ã£o. Ainda temos artistas de filmes (principalmente o gÃªnio Christopher Nolan). Mas esses dois caras simplesmente ... bem, eu tenho escrito palavras tÃ£o grandes, deixe-me colocar em termos sim...   \n",
       "4             Este filme eu vi um dia cedo de graÃ§a e ainda sinto que fui enganado. Ã totalmente morte cerebral. Arrotando, chutando a virilha e os peitos por todo o lugar. Coxo. O que hÃ¡ de errado com a sociedade, que filmes como esse sÃ£o feitos? As parÃ³dias eram todas horrendas e pouco engraÃ§adas. O enredo foi sem brilho, na melhor das hipÃ³teses, e a atuaÃ§Ã£o foi superficial, transparente e realmente bastante desnecessÃ¡ria. AlguÃ©m vÃª \"Idiocracia\"? Lembra do filme que ganhou todos os prÃªmios da academia no futuro? Bem, este Ã© esse filme. Eu nÃ£o vi um filme de baixa qualidade mais ranÃ§oso. \"Date Movie\" foi bom, The Scary Movies pelo menos teve enredos decentes, mas isso faz \"spoofs\" (se Ã© que posso dizer assim) para este ano 0 para 3, com \"Meet the Spartans\" e \"Filme de super-herÃ³is\" todos caindo. Bem, eu perdi ainda mais da minha vida digitando sobre esse saco de esterco de vaca. EntÃ£o, apesar de tudo, nÃ£o assista a este filme, a menos que o seu QI seja inferior a 80.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Reviews_clean  \n",
       "0                                                                                                                                                           watched conditional see films not caught dead giving hard earned money explain depth write shortest not see far unbelievably unfunny ever total since hatred others like extends far beyond one think go not know people besides carmen vanessa kim not though think editing flat possibly blatant continuity errors make crapfast even crappier thought know films not supposed serious come someone gets minor facial next someone gets cut blood least cut since narnia films away give disaster pass thoughtless mindless physical gags obviously take popular movies last year late including best picture know saddest thing stupid movies not care much money many cameos sorry ass excuses films taking away jobs directors truly deserve thought better taste ashamed making kind jason friedberg aaron burn guys contributing decline western cause downfall western  \n",
       "1                                                                                                                                                                                                                                                                                                                           writing hopes gets put previous review anyone find slop entertaining completely beyond first spoof entitled indeed spoof disaster seen count disaster iron alvin amy hancock register disaster selzterwater failburg shown lack sort writing skill unfortunately tortured date epic know exactly expect no jokes bad references cheaply remade scenes someone informed satire copy paste one though not say actually seem taken nothing clever witty remotely smart way two not believe people still pay see insult though enjoy films doubt smart enough realize unfortunately not number low enough includes rate deserves top worst films right date epic mean meet rather forced hour hands marathon watch  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            write scathing review turd going making observations points no point watching movies reader remember scary remember original comedic elements funny pretty forgettable worth price last time premise stop making call boycott pieces monkey know going line pre pubescent annoying little spouting crappy one liners rick james movies continue make form monetary considering production value looks like cost cents not see not spend money go rent laugh ass silently judge people talking  \n",
       "3  saw previous spoof movies two horrible know already tell want watch brainless person meant stereotypical laugh judge even automatically never ask comes two good old hollywood days whenever making showing people type also story kept edge well whenever word hit making films earned loads greedy people came picture quite two no still artists notably genius christopher two guys writing big let put simple terms guys not instead money craving latest proves even fails easily mind mean nothing funny people usually put best stuff idiots knew going made bet not good idea write reviews paper tell everyone whats good whats flipped review well warning not even called nothing artistic references made throughout pretty much like hannah montana juno gig actually close spoofing failed referencing instead joking twisting random wrestling not know high respect high respect know not something not add story nudity not really needed closest still gotten idea saw bare hate girl says guys perverts brainles...  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       saw day early free still feel like got ripped totally brain kicking groin boobs wrong films like even get parodies plot lackluster best acting transparent really quite see remember academy awards well not seen rancid crappy scary movies least decent makes nice call year falling wasted even life typing sack cow not see unless course iq r  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "#Data cleaning\n",
    "df['Reviews_clean']=df['Reviews'].apply(data_cleaning)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    60000\n",
      "1    60000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Mapping rating data to Binary label 1 (+ve) if rating >=7 and 0 (-ve) if rating <=4 and 2 (neutral) if rating = 5 or 6\n",
    "df['Label'] = df['Ratings'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))\n",
    "#Removing \n",
    "df=df[df.Label<'2']\n",
    "data=df[['Reviews_clean','Label']]\n",
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing dependencies for feature engineering \n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'are', 'sitting', 'on', 'the', 'mat', ',', 'and', 'they', 'are', 'purring', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatization of word \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wordnetlemma = WordNetLemmatizer()\n",
    "    def __call__(self, reviews):\n",
    "        return [self.wordnetlemma.lemmatize(word) for word in word_tokenize(reviews)]\n",
    "    \n",
    "# Create an instance of the LemmaTokenizer\n",
    "lemmatizer = LemmaTokenizer()\n",
    "\n",
    "# Example text to be lemmatized\n",
    "text = \"The cats are sitting on the mat, and they are purring.\"\n",
    "\n",
    "# Lemmatize the text\n",
    "lemmatized_words = lemmatizer(text)\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectoization with TFIDF Vectorizer with Unigram, Bigram and Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\ML projects\\sentiment_analysis\\env\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, test_size=.3, random_state=42, shuffle=True)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1, 3), min_df=10, max_features=500)\n",
    "\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with Chi squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t# Unigrams :\n",
      "\t. worst\n",
      "\t. great\n",
      "\t. waste\n",
      "\t. bad\n",
      "\t. loved\n",
      "\t. terrible\n",
      "\t. enjoyed\n",
      "\t. awful\n",
      "\t. poor\n",
      "\t. nothing\n",
      "\t. excellent\n",
      "\t. worse\n",
      "\t. best\n",
      "\t. boring\n",
      "\t. perfect\n",
      "\t. no\n",
      "\t. well\n",
      "\t. love\n",
      "\t. amazing\n",
      "\t. minute\n",
      "\t. definitely\n",
      "\t. money\n",
      "\t. even\n",
      "\t. horrible\n",
      "\t. fun\n",
      "\t. liked\n",
      "\t. wonderful\n",
      "\t. action\n",
      "\t. stupid\n",
      "\t. script\n",
      "\t. enjoy\n",
      "\t. favorite\n",
      "\t. enjoyable\n",
      "\t. cheap\n",
      "\t. highly\n",
      "\t. save\n",
      "\t. attempt\n",
      "\t. performance\n",
      "\t. job\n",
      "\t. beautiful\n",
      "\t. least\n",
      "\t. supposed\n",
      "\t. different\n",
      "\t. family\n",
      "\t. see\n",
      "\t. entertaining\n",
      "\t. also\n",
      "\t. surprised\n",
      "\t. acting\n",
      "\t. true\n",
      "\t. lack\n",
      "\t. recommend\n",
      "\t. role\n",
      "\t. heart\n",
      "\t. complete\n",
      "\t. always\n",
      "\t. hour\n",
      "\t. good\n",
      "\t. strong\n",
      "\t. bunch\n",
      "\t. lot\n",
      "\t. world\n",
      "\t. nice\n",
      "\t. none\n",
      "\t. classic\n",
      "\t. especially\n",
      "\t. ever\n",
      "\t. guess\n",
      "\t. instead\n",
      "\t. anything\n",
      "\t. fan\n",
      "\t. relationship\n",
      "\t. plenty\n",
      "\t. look\n",
      "\t. episode\n",
      "\t. comedy\n",
      "\t. dark\n",
      "\t. try\n",
      "\t. life\n",
      "\t. dialogue\n",
      "\t. humor\n",
      "\t. idea\n",
      "\t. comic\n",
      "\t. bit\n",
      "\t. trying\n",
      "\t. half\n",
      "\t. new\n",
      "\t. keep\n",
      "\t. twist\n",
      "\t. except\n",
      "\t. someone\n",
      "\t. reason\n",
      "\t. u\n",
      "\t. not\n",
      "\t. simple\n",
      "\t. title\n",
      "\t. decent\n",
      "\t. still\n",
      "\t. played\n",
      "\t. else\n",
      "\t. many\n",
      "\t. season\n",
      "\t. sound\n",
      "\t. camera\n",
      "\t. watching\n",
      "\t. may\n",
      "\t. saw\n",
      "\t. plot\n",
      "\t. writer\n",
      "\t. video\n",
      "\t. think\n",
      "\t. book\n",
      "\t. style\n",
      "\t. seems\n",
      "\t. thing\n",
      "\t. funny\n",
      "\t. theme\n",
      "\t. completely\n",
      "\t. basically\n",
      "\t. effort\n",
      "\t. production\n",
      "\t. right\n",
      "\t. experience\n",
      "\t. war\n",
      "\t. thought\n",
      "\t. play\n",
      "\t. wonder\n",
      "\t. cinematography\n",
      "\t. truly\n",
      "\t. human\n",
      "\t. looked\n",
      "\t. quite\n",
      "\t. story\n",
      "\t. john\n",
      "\t. name\n",
      "\t. seemed\n",
      "\t. although\n",
      "\t. easy\n",
      "\t. overall\n",
      "\t. review\n",
      "\t. writing\n",
      "\t. care\n",
      "\t. game\n",
      "\t. made\n",
      "\t. top\n",
      "\t. shot\n",
      "\t. fine\n",
      "\t. wanted\n",
      "\t. cast\n",
      "\t. cgi\n",
      "\t. problem\n",
      "\t. tv\n",
      "\t. actor\n",
      "\t. obvious\n",
      "\t. something\n",
      "\t. cool\n",
      "\t. give\n",
      "\t. kill\n",
      "\t. drama\n",
      "\t. joke\n",
      "\t. james\n",
      "\t. certainly\n",
      "\t. expecting\n",
      "\t. piece\n",
      "\t. monster\n",
      "\t. actually\n",
      "\t. single\n",
      "\t. open\n",
      "\t. seeing\n",
      "\t. started\n",
      "\t. worth\n",
      "\t. maybe\n",
      "\t. making\n",
      "\t. sometimes\n",
      "\t. beyond\n",
      "\t. people\n",
      "\t. fall\n",
      "\t. sort\n",
      "\t. theater\n",
      "\t. everyone\n",
      "\t. school\n",
      "\t. rest\n",
      "\t. score\n",
      "\t. along\n",
      "\t. running\n",
      "\t. side\n",
      "\t. change\n",
      "\t. moment\n",
      "\t. little\n",
      "\t. run\n",
      "\t. series\n",
      "\t. genre\n",
      "\t. hate\n",
      "\t. work\n",
      "\t. make\n",
      "\t. found\n",
      "\t. zombie\n",
      "\t. act\n",
      "\t. element\n",
      "\t. called\n",
      "\t. cut\n",
      "\t. others\n",
      "\t. young\n",
      "\t. last\n",
      "\t. quality\n",
      "\t. finally\n",
      "\t. audience\n",
      "\t. horror\n",
      "\t. live\n",
      "\t. man\n",
      "\t. budget\n",
      "\t. take\n",
      "\t. female\n",
      "\t. throughout\n",
      "\t. word\n",
      "\t. become\n",
      "\t. understand\n",
      "\t. cinema\n",
      "\t. based\n",
      "\t. either\n",
      "\t. guy\n",
      "\t. rather\n",
      "\t. head\n",
      "\t. expect\n",
      "\t. mind\n",
      "\t. effect\n",
      "\t. alien\n",
      "\t. sex\n",
      "\t. michael\n",
      "\t. can\n",
      "\t. woman\n",
      "\t. around\n",
      "\t. laugh\n",
      "\t. easily\n",
      "\t. day\n",
      "\t. like\n",
      "\t. silly\n",
      "\t. yet\n",
      "\t. serious\n",
      "\t. lost\n",
      "\t. girl\n",
      "\t. really\n",
      "\t. simply\n",
      "\t. back\n",
      "\t. hard\n",
      "\t. get\n",
      "\t. film\n",
      "\t. feel\n",
      "\t. question\n",
      "\t. certain\n",
      "\t. hold\n",
      "\t. bring\n",
      "\t. entire\n",
      "\t. totally\n",
      "\t. never\n",
      "\t. stand\n",
      "\t. read\n",
      "\t. full\n",
      "\t. believe\n",
      "\t. already\n",
      "\t. end\n",
      "\t. better\n",
      "\t. call\n",
      "\t. often\n",
      "\t. point\n",
      "\t. happens\n",
      "\t. talk\n",
      "\t. must\n",
      "\t. team\n",
      "\t. turned\n",
      "\t. kept\n",
      "\t. chance\n",
      "\t. meet\n",
      "\t. dead\n",
      "\t. coming\n",
      "\t. killer\n",
      "\t. written\n",
      "\t. first\n",
      "\t. line\n",
      "\t. aspect\n",
      "\t. enough\n",
      "\t. wrong\n",
      "\t. seem\n",
      "\t. song\n",
      "\t. find\n",
      "\t. star\n",
      "\t. typical\n",
      "\t. go\n",
      "\t. obviously\n",
      "\t. event\n",
      "\t. le\n",
      "\t. since\n",
      "\t. course\n",
      "\t. father\n",
      "\t. viewer\n",
      "\t. art\n",
      "\t. together\n",
      "\t. whole\n",
      "\t. hit\n",
      "\t. couple\n",
      "\t. almost\n",
      "\t. type\n",
      "\t. previous\n",
      "\t. hollywood\n",
      "\t. thinking\n",
      "\t. feeling\n",
      "\t. real\n",
      "\t. year\n",
      "\t. several\n",
      "\t. big\n",
      "\t. beginning\n",
      "\t. robert\n",
      "\t. similar\n",
      "\t. though\n",
      "\t. final\n",
      "\t. creature\n",
      "\t. left\n",
      "\t. talking\n",
      "\t. case\n",
      "\t. past\n",
      "\t. brother\n",
      "\t. picture\n",
      "\t. eye\n",
      "\t. stay\n",
      "\t. another\n",
      "\t. mean\n",
      "\t. used\n",
      "\t. including\n",
      "\t. start\n",
      "\t. music\n",
      "\t. whether\n",
      "\t. night\n",
      "\t. part\n",
      "\t. show\n",
      "\t. giving\n",
      "\t. hand\n",
      "\t. probably\n",
      "\t. american\n",
      "\t. hope\n",
      "\t. short\n",
      "\t. storyline\n",
      "\t. exactly\n",
      "\t. far\n",
      "\t. set\n",
      "\t. next\n",
      "\t. fact\n",
      "\t. given\n",
      "\t. movie\n",
      "\t. time\n",
      "\t. without\n",
      "\t. mostly\n",
      "\t. white\n",
      "\t. house\n",
      "\t. wife\n",
      "\t. help\n",
      "\t. use\n",
      "\t. boy\n",
      "\t. able\n",
      "\t. taken\n",
      "\t. got\n",
      "\t. low\n",
      "\t. child\n",
      "\t. taking\n",
      "\t. clearly\n",
      "\t. leave\n",
      "\t. old\n",
      "\t. said\n",
      "\t. friend\n",
      "\t. way\n",
      "\t. sure\n",
      "\t. group\n",
      "\t. absolutely\n",
      "\t. went\n",
      "\t. move\n",
      "\t. near\n",
      "\t. getting\n",
      "\t. original\n",
      "\t. felt\n",
      "\t. hero\n",
      "\t. put\n",
      "\t. across\n",
      "\t. second\n",
      "\t. amount\n",
      "\t. every\n",
      "\t. kid\n",
      "\t. special\n",
      "\t. small\n",
      "\t. stop\n",
      "\t. screen\n",
      "\t. slow\n",
      "\t. stuff\n",
      "\t. add\n",
      "\t. car\n",
      "\t. ending\n",
      "\t. death\n",
      "\t. gave\n",
      "\t. saying\n",
      "\t. rating\n",
      "\t. interesting\n",
      "\t. come\n",
      "\t. fight\n",
      "\t. sequel\n",
      "\t. sense\n",
      "\t. playing\n",
      "\t. gore\n",
      "\t. version\n",
      "\t. sequence\n",
      "\t. pretty\n",
      "\t. directed\n",
      "\t. face\n",
      "\t. black\n",
      "\t. place\n",
      "\t. need\n",
      "\t. direction\n",
      "\t. heard\n",
      "\t. kind\n",
      "\t. high\n",
      "\t. say\n",
      "\t. much\n",
      "\t. everything\n",
      "\t. feature\n",
      "\t. turn\n",
      "\t. becomes\n",
      "\t. scary\n",
      "\t. men\n",
      "\t. later\n",
      "\t. number\n",
      "\t. scene\n",
      "\t. perhaps\n",
      "\t. begin\n",
      "\t. going\n",
      "\t. lead\n",
      "\t. watched\n",
      "\t. done\n",
      "\t. voice\n",
      "\t. one\n",
      "\t. order\n",
      "\t. want\n",
      "\t. character\n",
      "\t. somewhat\n",
      "\t. watch\n",
      "\t. blood\n",
      "\t. home\n",
      "\t. opening\n",
      "\t. two\n",
      "\t. person\n",
      "\t. particularly\n",
      "\t. know\n",
      "\t. long\n",
      "\t. soon\n",
      "\t. remember\n",
      "\t. despite\n",
      "\t. dvd\n",
      "\t. close\n",
      "\t. however\n",
      "\t. knew\n",
      "\t. behind\n",
      "\t. wish\n",
      "\t. tell\n",
      "\t. matter\n",
      "\t. away\n",
      "\t. huge\n",
      "\t. looking\n",
      "\t. flick\n",
      "\t. extremely\n",
      "\t. early\n",
      "\t. seen\n",
      "\t. usually\n",
      "\t. level\n",
      "\t. evil\n",
      "\t. credit\n",
      "\t. known\n",
      "\t. took\n",
      "\t. three\n",
      "\t. anyone\n",
      "\t. came\n",
      "\t. due\n",
      "\t. main\n",
      "\t. major\n",
      "\t. let\n",
      "\t# Bigrams :\n",
      "\t. not even\n",
      "\t. one best\n",
      "\t. look like\n",
      "\t. not make\n",
      "\t. not like\n",
      "\t. not good\n",
      "\t. really not\n",
      "\t. not much\n",
      "\t. not not\n",
      "\t. not really\n",
      "\t. even though\n",
      "\t. low budget\n",
      "\t. can not\n",
      "\t. special effect\n",
      "\t. feel like\n",
      "\t. not get\n",
      "\t. not bad\n",
      "\t. not know\n",
      "\t. not think\n",
      "\t. not see\n",
      "\t# Trigrams :\n",
      "\t. \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "N = 5000\n",
    "featureselection = PrettyTable([\"Unigram\", \"Bigram\",\"Trigram\"])\n",
    "\n",
    "features_chi2 = chi2(x_train_tfidf, train['Label'])\n",
    "chi2score = features_chi2[0]\n",
    "\n",
    "scores = list(zip(tfidfvect.get_feature_names_out(), chi2score))\n",
    "sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "top_feature_names = [ngram for ngram, score in sorted_scores]\n",
    "\n",
    "unigrams = [x for x in top_feature_names if len(x.split(' ')) == 1]\n",
    "bigrams = [x for x in top_feature_names if len(x.split(' ')) == 2]\n",
    "trigrams = [x for x in top_feature_names if len(x.split(' ')) == 3]\n",
    "\n",
    "print(\"\\t# Unigrams :\\n\\t. %s\" %('\\n\\t. '.join(unigrams[:N])))\n",
    "print(\"\\t# Bigrams :\\n\\t. %s\" %('\\n\\t. '.join(bigrams[:N])))\n",
    "print(\"\\t# Trigrams :\\n\\t. %s\" %('\\n\\t. '.join(trigrams[:N])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for mapping model and f1 score\n",
    "model_to_f1_score = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation of Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score on training dateset for Logistic Regression: 0.8229285714285715\n",
      "AUC Score on training dateset for Logistic Regression: 0.9041354610928594\n",
      "F1 Score on training dateset for Logistic Regression: 0.8229197523419759\n",
      "\n",
      "\n",
      "Precision Score on test for Logistic Regression: 0.8163611111111111\n",
      "AUC Score on test for Logistic Regression: 0.8988959652367594\n",
      "F1 Score on test dataset for Logistic Regression: 0.8163628646101859\n"
     ]
    }
   ],
   "source": [
    "model_lgr = Pipeline(steps = [(\"classifier\", LogisticRegression())])\n",
    "model_lgr.fit(x_train_tfidf, y_train)\n",
    "\n",
    "train_prediction = model_lgr.predict(x_train_tfidf)\n",
    "\n",
    "print(\"Precision Score on training dateset for Logistic Regression: %s\" % precision_score(y_train, train_prediction, average='micro'))\n",
    "print(\"AUC Score on training dateset for Logistic Regression: %s\" % roc_auc_score(y_train, model_lgr.predict_proba(x_train_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_train_1 = f1_score(y_train, train_prediction, average=\"weighted\")\n",
    "print(\"F1 Score on training dateset for Logistic Regression: %s\" % f1_score_train_1)\n",
    "print(\"\\n\")\n",
    "\n",
    "test_prediction = model_lgr.predict(x_test_tfidf)\n",
    "\n",
    "print(\"Precision Score on test for Logistic Regression: %s\" % precision_score(y_test, test_prediction, average='micro'))\n",
    "print(\"AUC Score on test for Logistic Regression: %s\" % roc_auc_score(y_test, model_lgr.predict_proba(x_test_tfidf)[:,1], multi_class='ovo',average='macro'))\n",
    "\n",
    "f1_score_1 = f1_score(y_test, test_prediction, average=\"weighted\")\n",
    "print(\"F1 Score on test dataset for Logistic Regression: %s\" % f1_score_1)\n",
    "\n",
    "model_to_f1_score[model_lgr] = f1_score_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation of Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score on training dateset for Decision Tree Classifier: 0.9994642857142857\n",
      "AUC Score on training dateset for Decision Tree Classifier: 0.9999991357584328\n",
      "F1 Score training dateset for Decision Tree Classifier: 0.9994642865827811\n",
      "\n",
      "\n",
      "Precision Score on test for Decision Tree Classifier: 0.68475\n",
      "AUC Score on test for Decision Tree Classifier: 0.6850061146806661\n",
      "F1 Score for Decision Tree Classifier: 0.6847560227545438\n"
     ]
    }
   ],
   "source": [
    "model_dtc = Pipeline(\n",
    "    steps=[\n",
    "        #(\"classifier\", DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)),\n",
    "        (\"classifier\", DecisionTreeClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_dtc.fit(x_train_tfidf, y_train)\n",
    "\n",
    "train_prediction = model_dtc.predict(x_train_tfidf)\n",
    "\n",
    "print(\"Precision Score on training dateset for Decision Tree Classifier: %s\" % precision_score(y_train, train_prediction, average='micro'))\n",
    "print(\"AUC Score on training dateset for Decision Tree Classifier: %s\" % roc_auc_score(y_train, model_dtc.predict_proba(x_train_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_train_2 = f1_score(y_train, train_prediction, average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Decision Tree Classifier: %s\" % f1_score_train_2)\n",
    "print(\"\\n\")\n",
    "\n",
    "test_prediction = model_dtc.predict(x_test_tfidf)\n",
    "\n",
    "print(\"Precision Score on test for Decision Tree Classifier: %s\" % precision_score(y_test, test_prediction, average='micro'))\n",
    "print(\"AUC Score on test for Decision Tree Classifier: %s\" % roc_auc_score(y_test, model_dtc.predict_proba(x_test_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_2 = f1_score(y_test, test_prediction, average=\"weighted\")\n",
    "print(\"F1 Score for Decision Tree Classifier: %s\" % f1_score_2)\n",
    "\n",
    "model_to_f1_score[model_dtc] = f1_score_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier with max depth 11 to fix overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score on training dateset for Decision Tree Classifier: 0.7196309523809524\n",
      "AUC Score on training dateset for Decision Tree Classifier: 0.8067984314284834\n",
      "F1 Score training dateset for Decision Tree Classifier: 0.7148304951556239\n",
      "\n",
      "\n",
      "Precision Score on test for Decision Tree Classifier: 0.6889722222222222\n",
      "AUC Score on test for Decision Tree Classifier: 0.7611843173411232\n",
      "F1 Score for Decision Tree Classifier: 0.6837141805715267\n"
     ]
    }
   ],
   "source": [
    "model_dtc2 = Pipeline(\n",
    "    steps = [\n",
    "        (\"classifier\", DecisionTreeClassifier( criterion='gini', max_depth=11, min_samples_split=2, min_samples_leaf=1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_dtc2.fit(x_train_tfidf, y_train)\n",
    "\n",
    "train_prediction = model_dtc2.predict(x_train_tfidf)\n",
    "\n",
    "print(\"Precision Score on training dateset for Decision Tree Classifier: %s\" % precision_score(y_train, train_prediction, average='micro'))\n",
    "print(\"AUC Score on training dateset for Decision Tree Classifier: %s\" % roc_auc_score(y_train, model_dtc2.predict_proba(x_train_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_train_3 = f1_score(y_train, train_prediction, average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Decision Tree Classifier: %s\" % f1_score_train_3)\n",
    "print(\"\\n\")\n",
    "\n",
    "test_prediction = model_dtc2.predict(x_test_tfidf)\n",
    "\n",
    "print(\"Precision Score on test for Decision Tree Classifier: %s\" % precision_score(y_test, test_prediction, average='micro'))\n",
    "print(\"AUC Score on test for Decision Tree Classifier: %s\" % roc_auc_score(y_test, model_dtc2.predict_proba(x_test_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_3 = f1_score(y_test, test_prediction, average=\"weighted\")\n",
    "print(\"F1 Score for Decision Tree Classifier: %s\" % f1_score_3)\n",
    "\n",
    "model_to_f1_score[model_dtc2] = f1_score_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation of Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score on training dateset for Random Forest Classifier: 0.9994642857142857\n",
      "AUC Score on training dateset for Random Forest Classifier: 0.9999766155903397\n",
      "F1 Score training dateset for Random Forest Classifier: 0.99946428639449\n",
      "\n",
      "\n",
      "Precision Score on test for Random Forest Classifier: 0.7876388888888889\n",
      "AUC Score on test for Random Forest Classifier: 0.8717113528105923\n",
      "F1 Score for Random Forest Classifier: 0.7876409166457224\n"
     ]
    }
   ],
   "source": [
    "model_rfc = Pipeline(\n",
    "    steps=[\n",
    "        #(\"classifier\", RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=15, min_samples_split=3, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None))\n",
    "        (\"classifier\", RandomForestClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_rfc.fit(x_train_tfidf, y_train)\n",
    "\n",
    "train_prediction = model_rfc.predict(x_train_tfidf)\n",
    "\n",
    "print(\"Precision Score on training dateset for Random Forest Classifier: %s\" % precision_score(y_train, train_prediction, average='micro'))\n",
    "print(\"AUC Score on training dateset for Random Forest Classifier: %s\" % roc_auc_score(y_train, model_rfc.predict_proba(x_train_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_train_4 = f1_score(y_train, train_prediction, average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Random Forest Classifier: %s\" % f1_score_train_4)\n",
    "print(\"\\n\")\n",
    "\n",
    "test_prediction = model_rfc.predict(x_test_tfidf)\n",
    "\n",
    "print(\"Precision Score on test for Random Forest Classifier: %s\" % precision_score(y_test, model_rfc.predict(x_test_tfidf), average='micro'))\n",
    "print(\"AUC Score on test for Random Forest Classifier: %s\" % roc_auc_score(y_test, model_rfc.predict_proba(x_test_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_4 = f1_score(y_test,model_rfc.predict(x_test_tfidf), average=\"weighted\")\n",
    "print(\"F1 Score for Random Forest Classifier: %s\" % f1_score_4)\n",
    "\n",
    "model_to_f1_score[model_rfc] = f1_score_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation of Ada Boost (Adaptive Boost) Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\ML projects\\sentiment_analysis\\env\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score on training dateset for Ada Boost Classifier: 0.8490357142857143\n",
      "AUC Score on training dateset for Ada Boost Classifier: 0.9337366712918284\n",
      "F1 Score training dateset for Ada Boost Classifier: 0.8490315556668288\n",
      "\n",
      "\n",
      "Precision Score on test for Ada Boost Classifier: 0.79175\n",
      "AUC Score on test for Ada Boost Classifier: 0.8709099546136978\n",
      "F1 Score for Random Forest Classifier: 0.7917540672059571\n"
     ]
    }
   ],
   "source": [
    "model_abc = Pipeline(\n",
    "    steps = [\n",
    "        (\"classifier\", AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 4),\n",
    "        n_estimators = 100,\n",
    "        learning_rate = .8)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_abc.fit(x_train_tfidf, y_train)\n",
    "\n",
    "train_prediction = model_abc.predict(x_train_tfidf)\n",
    "\n",
    "print(\"Precision Score on training dateset for Ada Boost Classifier: %s\" % precision_score(y_train, train_prediction, average='micro'))\n",
    "print(\"AUC Score on training dateset for Ada Boost Classifier: %s\" % roc_auc_score(y_train, model_abc.predict_proba(x_train_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_train_5 = f1_score(y_train,train_prediction, average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Ada Boost Classifier: %s\" % f1_score_train_5)\n",
    "print(\"\\n\")\n",
    "\n",
    "test_prediction = model_abc.predict(x_test_tfidf)\n",
    "\n",
    "print(\"Precision Score on test for Ada Boost Classifier: %s\" % precision_score(y_test, test_prediction, average='micro'))\n",
    "print(\"AUC Score on test for Ada Boost Classifier: %s\" % roc_auc_score(y_test, model_abc.predict_proba(x_test_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_5 = f1_score(y_test, test_prediction, average=\"weighted\")\n",
    "print(\"F1 Score for Random Forest Classifier: %s\" % f1_score_5)\n",
    "\n",
    "model_to_f1_score[model_abc] = f1_score_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "def hyperparamtune(classifier, param_grid, metric, verbose_value, cv):\n",
    "    model = model_selection.GridSearchCV(\n",
    "            estimator = classifier,\n",
    "            param_grid = param_grid,\n",
    "            scoring = metric,\n",
    "            verbose = verbose_value,            \n",
    "            cv = cv)\n",
    "\n",
    "    model.fit(x_train_tfidf, y_train)\n",
    "    print(\"Best Score %s\" % {model.best_score_})\n",
    "\n",
    "    print(\"Best hyperparameter set:\")\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n",
    "\n",
    "    return model, best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tunning of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV 1/5; 1/48] START C=0.01, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 1/5; 1/48] END C=0.01, max_iter=100, penalty=l2, tol=0.0001;, score=0.805 total time=   0.6s\n",
      "[CV 2/5; 1/48] START C=0.01, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 2/5; 1/48] END C=0.01, max_iter=100, penalty=l2, tol=0.0001;, score=0.807 total time=   0.7s\n",
      "[CV 3/5; 1/48] START C=0.01, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 3/5; 1/48] END C=0.01, max_iter=100, penalty=l2, tol=0.0001;, score=0.811 total time=   0.7s\n",
      "[CV 4/5; 1/48] START C=0.01, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 4/5; 1/48] END C=0.01, max_iter=100, penalty=l2, tol=0.0001;, score=0.807 total time=   0.6s\n",
      "[CV 5/5; 1/48] START C=0.01, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 5/5; 1/48] END C=0.01, max_iter=100, penalty=l2, tol=0.0001;, score=0.805 total time=   0.6s\n",
      "[CV 1/5; 2/48] START C=0.01, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 1/5; 2/48] END C=0.01, max_iter=100, penalty=l2, tol=0.001;, score=0.805 total time=   0.5s\n",
      "[CV 2/5; 2/48] START C=0.01, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 2/5; 2/48] END C=0.01, max_iter=100, penalty=l2, tol=0.001;, score=0.804 total time=   0.6s\n",
      "[CV 3/5; 2/48] START C=0.01, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 3/5; 2/48] END C=0.01, max_iter=100, penalty=l2, tol=0.001;, score=0.810 total time=   0.5s\n",
      "[CV 4/5; 2/48] START C=0.01, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 4/5; 2/48] END C=0.01, max_iter=100, penalty=l2, tol=0.001;, score=0.806 total time=   0.9s\n",
      "[CV 5/5; 2/48] START C=0.01, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 5/5; 2/48] END C=0.01, max_iter=100, penalty=l2, tol=0.001;, score=0.805 total time=   1.6s\n",
      "[CV 1/5; 3/48] START C=0.01, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 1/5; 3/48] END C=0.01, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   1.0s\n",
      "[CV 2/5; 3/48] START C=0.01, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 2/5; 3/48] END C=0.01, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.5s\n",
      "[CV 3/5; 3/48] START C=0.01, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 3/5; 3/48] END C=0.01, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 4/5; 3/48] START C=0.01, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 4/5; 3/48] END C=0.01, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 5/5; 3/48] START C=0.01, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 5/5; 3/48] END C=0.01, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   1.2s\n",
      "[CV 1/5; 4/48] START C=0.01, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 1/5; 4/48] END C=0.01, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 4/48] START C=0.01, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 2/5; 4/48] END C=0.01, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 4/48] START C=0.01, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 3/5; 4/48] END C=0.01, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 4/48] START C=0.01, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 4/5; 4/48] END C=0.01, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 4/48] START C=0.01, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 5/5; 4/48] END C=0.01, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 5/48] START C=0.01, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 1/5; 5/48] END C=0.01, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 5/48] START C=0.01, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 2/5; 5/48] END C=0.01, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 5/48] START C=0.01, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 3/5; 5/48] END C=0.01, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 5/48] START C=0.01, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 4/5; 5/48] END C=0.01, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 5/48] START C=0.01, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 5/5; 5/48] END C=0.01, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 6/48] START C=0.01, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 1/5; 6/48] END C=0.01, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 2/5; 6/48] START C=0.01, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 2/5; 6/48] END C=0.01, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 3/5; 6/48] START C=0.01, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 3/5; 6/48] END C=0.01, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 4/5; 6/48] START C=0.01, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 4/5; 6/48] END C=0.01, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 5/5; 6/48] START C=0.01, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 5/5; 6/48] END C=0.01, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 1/5; 7/48] START C=0.01, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 1/5; 7/48] END C=0.01, max_iter=200, penalty=l2, tol=0.0001;, score=0.805 total time=   0.7s\n",
      "[CV 2/5; 7/48] START C=0.01, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 2/5; 7/48] END C=0.01, max_iter=200, penalty=l2, tol=0.0001;, score=0.807 total time=   0.8s\n",
      "[CV 3/5; 7/48] START C=0.01, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 3/5; 7/48] END C=0.01, max_iter=200, penalty=l2, tol=0.0001;, score=0.811 total time=   0.6s\n",
      "[CV 4/5; 7/48] START C=0.01, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 4/5; 7/48] END C=0.01, max_iter=200, penalty=l2, tol=0.0001;, score=0.807 total time=   0.6s\n",
      "[CV 5/5; 7/48] START C=0.01, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 5/5; 7/48] END C=0.01, max_iter=200, penalty=l2, tol=0.0001;, score=0.805 total time=   0.7s\n",
      "[CV 1/5; 8/48] START C=0.01, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 1/5; 8/48] END C=0.01, max_iter=200, penalty=l2, tol=0.001;, score=0.805 total time=   0.5s\n",
      "[CV 2/5; 8/48] START C=0.01, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 2/5; 8/48] END C=0.01, max_iter=200, penalty=l2, tol=0.001;, score=0.804 total time=   0.4s\n",
      "[CV 3/5; 8/48] START C=0.01, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 3/5; 8/48] END C=0.01, max_iter=200, penalty=l2, tol=0.001;, score=0.810 total time=   0.5s\n",
      "[CV 4/5; 8/48] START C=0.01, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 4/5; 8/48] END C=0.01, max_iter=200, penalty=l2, tol=0.001;, score=0.806 total time=   0.6s\n",
      "[CV 5/5; 8/48] START C=0.01, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 5/5; 8/48] END C=0.01, max_iter=200, penalty=l2, tol=0.001;, score=0.805 total time=   0.8s\n",
      "[CV 1/5; 9/48] START C=0.01, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 1/5; 9/48] END C=0.01, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.8s\n",
      "[CV 2/5; 9/48] START C=0.01, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 2/5; 9/48] END C=0.01, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 3/5; 9/48] START C=0.01, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 3/5; 9/48] END C=0.01, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 4/5; 9/48] START C=0.01, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 4/5; 9/48] END C=0.01, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 5/5; 9/48] START C=0.01, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 5/5; 9/48] END C=0.01, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 10/48] START C=0.01, max_iter=200, penalty=l1, tol=0.0001..............\n",
      "[CV 1/5; 10/48] END C=0.01, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 10/48] START C=0.01, max_iter=200, penalty=l1, tol=0.0001..............\n",
      "[CV 2/5; 10/48] END C=0.01, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 10/48] START C=0.01, max_iter=200, penalty=l1, tol=0.0001..............\n",
      "[CV 3/5; 10/48] END C=0.01, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 10/48] START C=0.01, max_iter=200, penalty=l1, tol=0.0001..............\n",
      "[CV 4/5; 10/48] END C=0.01, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 10/48] START C=0.01, max_iter=200, penalty=l1, tol=0.0001..............\n",
      "[CV 5/5; 10/48] END C=0.01, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 11/48] START C=0.01, max_iter=200, penalty=l1, tol=0.001...............\n",
      "[CV 1/5; 11/48] END C=0.01, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 11/48] START C=0.01, max_iter=200, penalty=l1, tol=0.001...............\n",
      "[CV 2/5; 11/48] END C=0.01, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 11/48] START C=0.01, max_iter=200, penalty=l1, tol=0.001...............\n",
      "[CV 3/5; 11/48] END C=0.01, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 11/48] START C=0.01, max_iter=200, penalty=l1, tol=0.001...............\n",
      "[CV 4/5; 11/48] END C=0.01, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 11/48] START C=0.01, max_iter=200, penalty=l1, tol=0.001...............\n",
      "[CV 5/5; 11/48] END C=0.01, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 12/48] START C=0.01, max_iter=200, penalty=l1, tol=0.01................\n",
      "[CV 1/5; 12/48] END C=0.01, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 2/5; 12/48] START C=0.01, max_iter=200, penalty=l1, tol=0.01................\n",
      "[CV 2/5; 12/48] END C=0.01, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 3/5; 12/48] START C=0.01, max_iter=200, penalty=l1, tol=0.01................\n",
      "[CV 3/5; 12/48] END C=0.01, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 4/5; 12/48] START C=0.01, max_iter=200, penalty=l1, tol=0.01................\n",
      "[CV 4/5; 12/48] END C=0.01, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 5/5; 12/48] START C=0.01, max_iter=200, penalty=l1, tol=0.01................\n",
      "[CV 5/5; 12/48] END C=0.01, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 1/5; 13/48] START C=0.1, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 1/5; 13/48] END C=0.1, max_iter=100, penalty=l2, tol=0.0001;, score=0.819 total time=   0.8s\n",
      "[CV 2/5; 13/48] START C=0.1, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 2/5; 13/48] END C=0.1, max_iter=100, penalty=l2, tol=0.0001;, score=0.819 total time=   0.9s\n",
      "[CV 3/5; 13/48] START C=0.1, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 3/5; 13/48] END C=0.1, max_iter=100, penalty=l2, tol=0.0001;, score=0.820 total time=   1.0s\n",
      "[CV 4/5; 13/48] START C=0.1, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 4/5; 13/48] END C=0.1, max_iter=100, penalty=l2, tol=0.0001;, score=0.814 total time=   1.1s\n",
      "[CV 5/5; 13/48] START C=0.1, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 5/5; 13/48] END C=0.1, max_iter=100, penalty=l2, tol=0.0001;, score=0.819 total time=   1.0s\n",
      "[CV 1/5; 14/48] START C=0.1, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 1/5; 14/48] END C=0.1, max_iter=100, penalty=l2, tol=0.001;, score=0.819 total time=   0.7s\n",
      "[CV 2/5; 14/48] START C=0.1, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 2/5; 14/48] END C=0.1, max_iter=100, penalty=l2, tol=0.001;, score=0.819 total time=   1.0s\n",
      "[CV 3/5; 14/48] START C=0.1, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 3/5; 14/48] END C=0.1, max_iter=100, penalty=l2, tol=0.001;, score=0.819 total time=   0.6s\n",
      "[CV 4/5; 14/48] START C=0.1, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 4/5; 14/48] END C=0.1, max_iter=100, penalty=l2, tol=0.001;, score=0.815 total time=   0.6s\n",
      "[CV 5/5; 14/48] START C=0.1, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 5/5; 14/48] END C=0.1, max_iter=100, penalty=l2, tol=0.001;, score=0.818 total time=   0.5s\n",
      "[CV 1/5; 15/48] START C=0.1, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 1/5; 15/48] END C=0.1, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 2/5; 15/48] START C=0.1, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 2/5; 15/48] END C=0.1, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 3/5; 15/48] START C=0.1, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 3/5; 15/48] END C=0.1, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 4/5; 15/48] START C=0.1, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 4/5; 15/48] END C=0.1, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 5/5; 15/48] START C=0.1, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 5/5; 15/48] END C=0.1, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 16/48] START C=0.1, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 1/5; 16/48] END C=0.1, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 16/48] START C=0.1, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 2/5; 16/48] END C=0.1, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 16/48] START C=0.1, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 3/5; 16/48] END C=0.1, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 16/48] START C=0.1, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 4/5; 16/48] END C=0.1, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 16/48] START C=0.1, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 5/5; 16/48] END C=0.1, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 17/48] START C=0.1, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 1/5; 17/48] END C=0.1, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 17/48] START C=0.1, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 2/5; 17/48] END C=0.1, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 17/48] START C=0.1, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 3/5; 17/48] END C=0.1, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 17/48] START C=0.1, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 4/5; 17/48] END C=0.1, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 17/48] START C=0.1, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 5/5; 17/48] END C=0.1, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 18/48] START C=0.1, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 1/5; 18/48] END C=0.1, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 2/5; 18/48] START C=0.1, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 2/5; 18/48] END C=0.1, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 3/5; 18/48] START C=0.1, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 3/5; 18/48] END C=0.1, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 4/5; 18/48] START C=0.1, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 4/5; 18/48] END C=0.1, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 5/5; 18/48] START C=0.1, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 5/5; 18/48] END C=0.1, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 1/5; 19/48] START C=0.1, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 1/5; 19/48] END C=0.1, max_iter=200, penalty=l2, tol=0.0001;, score=0.819 total time=   0.8s\n",
      "[CV 2/5; 19/48] START C=0.1, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 2/5; 19/48] END C=0.1, max_iter=200, penalty=l2, tol=0.0001;, score=0.819 total time=   1.0s\n",
      "[CV 3/5; 19/48] START C=0.1, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 3/5; 19/48] END C=0.1, max_iter=200, penalty=l2, tol=0.0001;, score=0.820 total time=   1.4s\n",
      "[CV 4/5; 19/48] START C=0.1, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 4/5; 19/48] END C=0.1, max_iter=200, penalty=l2, tol=0.0001;, score=0.814 total time=   0.7s\n",
      "[CV 5/5; 19/48] START C=0.1, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 5/5; 19/48] END C=0.1, max_iter=200, penalty=l2, tol=0.0001;, score=0.819 total time=   0.7s\n",
      "[CV 1/5; 20/48] START C=0.1, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 1/5; 20/48] END C=0.1, max_iter=200, penalty=l2, tol=0.001;, score=0.819 total time=   0.6s\n",
      "[CV 2/5; 20/48] START C=0.1, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 2/5; 20/48] END C=0.1, max_iter=200, penalty=l2, tol=0.001;, score=0.819 total time=   0.5s\n",
      "[CV 3/5; 20/48] START C=0.1, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 3/5; 20/48] END C=0.1, max_iter=200, penalty=l2, tol=0.001;, score=0.819 total time=   0.5s\n",
      "[CV 4/5; 20/48] START C=0.1, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 4/5; 20/48] END C=0.1, max_iter=200, penalty=l2, tol=0.001;, score=0.815 total time=   0.6s\n",
      "[CV 5/5; 20/48] START C=0.1, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 5/5; 20/48] END C=0.1, max_iter=200, penalty=l2, tol=0.001;, score=0.818 total time=   0.6s\n",
      "[CV 1/5; 21/48] START C=0.1, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 1/5; 21/48] END C=0.1, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 2/5; 21/48] START C=0.1, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 2/5; 21/48] END C=0.1, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 3/5; 21/48] START C=0.1, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 3/5; 21/48] END C=0.1, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 4/5; 21/48] START C=0.1, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 4/5; 21/48] END C=0.1, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.5s\n",
      "[CV 5/5; 21/48] START C=0.1, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 5/5; 21/48] END C=0.1, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.6s\n",
      "[CV 1/5; 22/48] START C=0.1, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 1/5; 22/48] END C=0.1, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.1s\n",
      "[CV 2/5; 22/48] START C=0.1, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 2/5; 22/48] END C=0.1, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 22/48] START C=0.1, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 3/5; 22/48] END C=0.1, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 22/48] START C=0.1, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 4/5; 22/48] END C=0.1, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 22/48] START C=0.1, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 5/5; 22/48] END C=0.1, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 23/48] START C=0.1, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 1/5; 23/48] END C=0.1, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 23/48] START C=0.1, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 2/5; 23/48] END C=0.1, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 23/48] START C=0.1, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 3/5; 23/48] END C=0.1, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 23/48] START C=0.1, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 4/5; 23/48] END C=0.1, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 23/48] START C=0.1, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 5/5; 23/48] END C=0.1, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 24/48] START C=0.1, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 1/5; 24/48] END C=0.1, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 2/5; 24/48] START C=0.1, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 2/5; 24/48] END C=0.1, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 3/5; 24/48] START C=0.1, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 3/5; 24/48] END C=0.1, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 4/5; 24/48] START C=0.1, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 4/5; 24/48] END C=0.1, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 5/5; 24/48] START C=0.1, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 5/5; 24/48] END C=0.1, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 1/5; 25/48] START C=1.0, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 1/5; 25/48] END C=1.0, max_iter=100, penalty=l2, tol=0.0001;, score=0.819 total time=   0.8s\n",
      "[CV 2/5; 25/48] START C=1.0, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 2/5; 25/48] END C=1.0, max_iter=100, penalty=l2, tol=0.0001;, score=0.821 total time=   0.9s\n",
      "[CV 3/5; 25/48] START C=1.0, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 3/5; 25/48] END C=1.0, max_iter=100, penalty=l2, tol=0.0001;, score=0.819 total time=   0.7s\n",
      "[CV 4/5; 25/48] START C=1.0, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 4/5; 25/48] END C=1.0, max_iter=100, penalty=l2, tol=0.0001;, score=0.815 total time=   0.8s\n",
      "[CV 5/5; 25/48] START C=1.0, max_iter=100, penalty=l2, tol=0.0001...............\n",
      "[CV 5/5; 25/48] END C=1.0, max_iter=100, penalty=l2, tol=0.0001;, score=0.820 total time=   0.8s\n",
      "[CV 1/5; 26/48] START C=1.0, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 1/5; 26/48] END C=1.0, max_iter=100, penalty=l2, tol=0.001;, score=0.819 total time=   0.5s\n",
      "[CV 2/5; 26/48] START C=1.0, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 2/5; 26/48] END C=1.0, max_iter=100, penalty=l2, tol=0.001;, score=0.821 total time=   0.7s\n",
      "[CV 3/5; 26/48] START C=1.0, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 3/5; 26/48] END C=1.0, max_iter=100, penalty=l2, tol=0.001;, score=0.821 total time=   0.6s\n",
      "[CV 4/5; 26/48] START C=1.0, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 4/5; 26/48] END C=1.0, max_iter=100, penalty=l2, tol=0.001;, score=0.816 total time=   0.7s\n",
      "[CV 5/5; 26/48] START C=1.0, max_iter=100, penalty=l2, tol=0.001................\n",
      "[CV 5/5; 26/48] END C=1.0, max_iter=100, penalty=l2, tol=0.001;, score=0.820 total time=   0.8s\n",
      "[CV 1/5; 27/48] START C=1.0, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 1/5; 27/48] END C=1.0, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 2/5; 27/48] START C=1.0, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 2/5; 27/48] END C=1.0, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.4s\n",
      "[CV 3/5; 27/48] START C=1.0, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 3/5; 27/48] END C=1.0, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 4/5; 27/48] START C=1.0, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 4/5; 27/48] END C=1.0, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 5/5; 27/48] START C=1.0, max_iter=100, penalty=l2, tol=0.01.................\n",
      "[CV 5/5; 27/48] END C=1.0, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 28/48] START C=1.0, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 1/5; 28/48] END C=1.0, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 28/48] START C=1.0, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 2/5; 28/48] END C=1.0, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 28/48] START C=1.0, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 3/5; 28/48] END C=1.0, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 28/48] START C=1.0, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 4/5; 28/48] END C=1.0, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 28/48] START C=1.0, max_iter=100, penalty=l1, tol=0.0001...............\n",
      "[CV 5/5; 28/48] END C=1.0, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 29/48] START C=1.0, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 1/5; 29/48] END C=1.0, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 29/48] START C=1.0, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 2/5; 29/48] END C=1.0, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 29/48] START C=1.0, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 3/5; 29/48] END C=1.0, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 29/48] START C=1.0, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 4/5; 29/48] END C=1.0, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 29/48] START C=1.0, max_iter=100, penalty=l1, tol=0.001................\n",
      "[CV 5/5; 29/48] END C=1.0, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 30/48] START C=1.0, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 1/5; 30/48] END C=1.0, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 2/5; 30/48] START C=1.0, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 2/5; 30/48] END C=1.0, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 3/5; 30/48] START C=1.0, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 3/5; 30/48] END C=1.0, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 4/5; 30/48] START C=1.0, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 4/5; 30/48] END C=1.0, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 5/5; 30/48] START C=1.0, max_iter=100, penalty=l1, tol=0.01.................\n",
      "[CV 5/5; 30/48] END C=1.0, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 1/5; 31/48] START C=1.0, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 1/5; 31/48] END C=1.0, max_iter=200, penalty=l2, tol=0.0001;, score=0.819 total time=   0.8s\n",
      "[CV 2/5; 31/48] START C=1.0, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 2/5; 31/48] END C=1.0, max_iter=200, penalty=l2, tol=0.0001;, score=0.821 total time=   0.8s\n",
      "[CV 3/5; 31/48] START C=1.0, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 3/5; 31/48] END C=1.0, max_iter=200, penalty=l2, tol=0.0001;, score=0.819 total time=   0.7s\n",
      "[CV 4/5; 31/48] START C=1.0, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 4/5; 31/48] END C=1.0, max_iter=200, penalty=l2, tol=0.0001;, score=0.815 total time=   1.1s\n",
      "[CV 5/5; 31/48] START C=1.0, max_iter=200, penalty=l2, tol=0.0001...............\n",
      "[CV 5/5; 31/48] END C=1.0, max_iter=200, penalty=l2, tol=0.0001;, score=0.820 total time=   0.9s\n",
      "[CV 1/5; 32/48] START C=1.0, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 1/5; 32/48] END C=1.0, max_iter=200, penalty=l2, tol=0.001;, score=0.819 total time=   0.6s\n",
      "[CV 2/5; 32/48] START C=1.0, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 2/5; 32/48] END C=1.0, max_iter=200, penalty=l2, tol=0.001;, score=0.821 total time=   0.9s\n",
      "[CV 3/5; 32/48] START C=1.0, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 3/5; 32/48] END C=1.0, max_iter=200, penalty=l2, tol=0.001;, score=0.821 total time=   0.9s\n",
      "[CV 4/5; 32/48] START C=1.0, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 4/5; 32/48] END C=1.0, max_iter=200, penalty=l2, tol=0.001;, score=0.816 total time=   0.5s\n",
      "[CV 5/5; 32/48] START C=1.0, max_iter=200, penalty=l2, tol=0.001................\n",
      "[CV 5/5; 32/48] END C=1.0, max_iter=200, penalty=l2, tol=0.001;, score=0.820 total time=   0.6s\n",
      "[CV 1/5; 33/48] START C=1.0, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 1/5; 33/48] END C=1.0, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 2/5; 33/48] START C=1.0, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 2/5; 33/48] END C=1.0, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 3/5; 33/48] START C=1.0, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 3/5; 33/48] END C=1.0, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 4/5; 33/48] START C=1.0, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 4/5; 33/48] END C=1.0, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 5/5; 33/48] START C=1.0, max_iter=200, penalty=l2, tol=0.01.................\n",
      "[CV 5/5; 33/48] END C=1.0, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 34/48] START C=1.0, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 1/5; 34/48] END C=1.0, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 34/48] START C=1.0, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 2/5; 34/48] END C=1.0, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 34/48] START C=1.0, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 3/5; 34/48] END C=1.0, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 34/48] START C=1.0, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 4/5; 34/48] END C=1.0, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 34/48] START C=1.0, max_iter=200, penalty=l1, tol=0.0001...............\n",
      "[CV 5/5; 34/48] END C=1.0, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 35/48] START C=1.0, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 1/5; 35/48] END C=1.0, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 35/48] START C=1.0, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 2/5; 35/48] END C=1.0, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 35/48] START C=1.0, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 3/5; 35/48] END C=1.0, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 35/48] START C=1.0, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 4/5; 35/48] END C=1.0, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 35/48] START C=1.0, max_iter=200, penalty=l1, tol=0.001................\n",
      "[CV 5/5; 35/48] END C=1.0, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 36/48] START C=1.0, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 1/5; 36/48] END C=1.0, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 2/5; 36/48] START C=1.0, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 2/5; 36/48] END C=1.0, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 3/5; 36/48] START C=1.0, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 3/5; 36/48] END C=1.0, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 4/5; 36/48] START C=1.0, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 4/5; 36/48] END C=1.0, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 5/5; 36/48] START C=1.0, max_iter=200, penalty=l1, tol=0.01.................\n",
      "[CV 5/5; 36/48] END C=1.0, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 1/5; 37/48] START C=10, max_iter=100, penalty=l2, tol=0.0001................\n",
      "[CV 1/5; 37/48] END C=10, max_iter=100, penalty=l2, tol=0.0001;, score=0.818 total time=   0.8s\n",
      "[CV 2/5; 37/48] START C=10, max_iter=100, penalty=l2, tol=0.0001................\n",
      "[CV 2/5; 37/48] END C=10, max_iter=100, penalty=l2, tol=0.0001;, score=0.822 total time=   1.6s\n",
      "[CV 3/5; 37/48] START C=10, max_iter=100, penalty=l2, tol=0.0001................\n",
      "[CV 3/5; 37/48] END C=10, max_iter=100, penalty=l2, tol=0.0001;, score=0.819 total time=   1.0s\n",
      "[CV 4/5; 37/48] START C=10, max_iter=100, penalty=l2, tol=0.0001................\n",
      "[CV 4/5; 37/48] END C=10, max_iter=100, penalty=l2, tol=0.0001;, score=0.815 total time=   1.5s\n",
      "[CV 5/5; 37/48] START C=10, max_iter=100, penalty=l2, tol=0.0001................\n",
      "[CV 5/5; 37/48] END C=10, max_iter=100, penalty=l2, tol=0.0001;, score=0.819 total time=   1.0s\n",
      "[CV 1/5; 38/48] START C=10, max_iter=100, penalty=l2, tol=0.001.................\n",
      "[CV 1/5; 38/48] END C=10, max_iter=100, penalty=l2, tol=0.001;, score=0.819 total time=   0.6s\n",
      "[CV 2/5; 38/48] START C=10, max_iter=100, penalty=l2, tol=0.001.................\n",
      "[CV 2/5; 38/48] END C=10, max_iter=100, penalty=l2, tol=0.001;, score=0.821 total time=   0.7s\n",
      "[CV 3/5; 38/48] START C=10, max_iter=100, penalty=l2, tol=0.001.................\n",
      "[CV 3/5; 38/48] END C=10, max_iter=100, penalty=l2, tol=0.001;, score=0.820 total time=   0.6s\n",
      "[CV 4/5; 38/48] START C=10, max_iter=100, penalty=l2, tol=0.001.................\n",
      "[CV 4/5; 38/48] END C=10, max_iter=100, penalty=l2, tol=0.001;, score=0.816 total time=   0.6s\n",
      "[CV 5/5; 38/48] START C=10, max_iter=100, penalty=l2, tol=0.001.................\n",
      "[CV 5/5; 38/48] END C=10, max_iter=100, penalty=l2, tol=0.001;, score=0.819 total time=   0.6s\n",
      "[CV 1/5; 39/48] START C=10, max_iter=100, penalty=l2, tol=0.01..................\n",
      "[CV 1/5; 39/48] END C=10, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 2/5; 39/48] START C=10, max_iter=100, penalty=l2, tol=0.01..................\n",
      "[CV 2/5; 39/48] END C=10, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 3/5; 39/48] START C=10, max_iter=100, penalty=l2, tol=0.01..................\n",
      "[CV 3/5; 39/48] END C=10, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 4/5; 39/48] START C=10, max_iter=100, penalty=l2, tol=0.01..................\n",
      "[CV 4/5; 39/48] END C=10, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.3s\n",
      "[CV 5/5; 39/48] START C=10, max_iter=100, penalty=l2, tol=0.01..................\n",
      "[CV 5/5; 39/48] END C=10, max_iter=100, penalty=l2, tol=0.01;, score=0.498 total time=   0.6s\n",
      "[CV 1/5; 40/48] START C=10, max_iter=100, penalty=l1, tol=0.0001................\n",
      "[CV 1/5; 40/48] END C=10, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.3s\n",
      "[CV 2/5; 40/48] START C=10, max_iter=100, penalty=l1, tol=0.0001................\n",
      "[CV 2/5; 40/48] END C=10, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.1s\n",
      "[CV 3/5; 40/48] START C=10, max_iter=100, penalty=l1, tol=0.0001................\n",
      "[CV 3/5; 40/48] END C=10, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.1s\n",
      "[CV 4/5; 40/48] START C=10, max_iter=100, penalty=l1, tol=0.0001................\n",
      "[CV 4/5; 40/48] END C=10, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 40/48] START C=10, max_iter=100, penalty=l1, tol=0.0001................\n",
      "[CV 5/5; 40/48] END C=10, max_iter=100, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 41/48] START C=10, max_iter=100, penalty=l1, tol=0.001.................\n",
      "[CV 1/5; 41/48] END C=10, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 41/48] START C=10, max_iter=100, penalty=l1, tol=0.001.................\n",
      "[CV 2/5; 41/48] END C=10, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 41/48] START C=10, max_iter=100, penalty=l1, tol=0.001.................\n",
      "[CV 3/5; 41/48] END C=10, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 41/48] START C=10, max_iter=100, penalty=l1, tol=0.001.................\n",
      "[CV 4/5; 41/48] END C=10, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 41/48] START C=10, max_iter=100, penalty=l1, tol=0.001.................\n",
      "[CV 5/5; 41/48] END C=10, max_iter=100, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 42/48] START C=10, max_iter=100, penalty=l1, tol=0.01..................\n",
      "[CV 1/5; 42/48] END C=10, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 2/5; 42/48] START C=10, max_iter=100, penalty=l1, tol=0.01..................\n",
      "[CV 2/5; 42/48] END C=10, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 3/5; 42/48] START C=10, max_iter=100, penalty=l1, tol=0.01..................\n",
      "[CV 3/5; 42/48] END C=10, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 4/5; 42/48] START C=10, max_iter=100, penalty=l1, tol=0.01..................\n",
      "[CV 4/5; 42/48] END C=10, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 5/5; 42/48] START C=10, max_iter=100, penalty=l1, tol=0.01..................\n",
      "[CV 5/5; 42/48] END C=10, max_iter=100, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 1/5; 43/48] START C=10, max_iter=200, penalty=l2, tol=0.0001................\n",
      "[CV 1/5; 43/48] END C=10, max_iter=200, penalty=l2, tol=0.0001;, score=0.818 total time=   1.0s\n",
      "[CV 2/5; 43/48] START C=10, max_iter=200, penalty=l2, tol=0.0001................\n",
      "[CV 2/5; 43/48] END C=10, max_iter=200, penalty=l2, tol=0.0001;, score=0.822 total time=   1.1s\n",
      "[CV 3/5; 43/48] START C=10, max_iter=200, penalty=l2, tol=0.0001................\n",
      "[CV 3/5; 43/48] END C=10, max_iter=200, penalty=l2, tol=0.0001;, score=0.819 total time=   0.7s\n",
      "[CV 4/5; 43/48] START C=10, max_iter=200, penalty=l2, tol=0.0001................\n",
      "[CV 4/5; 43/48] END C=10, max_iter=200, penalty=l2, tol=0.0001;, score=0.815 total time=   1.1s\n",
      "[CV 5/5; 43/48] START C=10, max_iter=200, penalty=l2, tol=0.0001................\n",
      "[CV 5/5; 43/48] END C=10, max_iter=200, penalty=l2, tol=0.0001;, score=0.819 total time=   1.0s\n",
      "[CV 1/5; 44/48] START C=10, max_iter=200, penalty=l2, tol=0.001.................\n",
      "[CV 1/5; 44/48] END C=10, max_iter=200, penalty=l2, tol=0.001;, score=0.819 total time=   0.8s\n",
      "[CV 2/5; 44/48] START C=10, max_iter=200, penalty=l2, tol=0.001.................\n",
      "[CV 2/5; 44/48] END C=10, max_iter=200, penalty=l2, tol=0.001;, score=0.821 total time=   0.7s\n",
      "[CV 3/5; 44/48] START C=10, max_iter=200, penalty=l2, tol=0.001.................\n",
      "[CV 3/5; 44/48] END C=10, max_iter=200, penalty=l2, tol=0.001;, score=0.820 total time=   0.7s\n",
      "[CV 4/5; 44/48] START C=10, max_iter=200, penalty=l2, tol=0.001.................\n",
      "[CV 4/5; 44/48] END C=10, max_iter=200, penalty=l2, tol=0.001;, score=0.816 total time=   0.6s\n",
      "[CV 5/5; 44/48] START C=10, max_iter=200, penalty=l2, tol=0.001.................\n",
      "[CV 5/5; 44/48] END C=10, max_iter=200, penalty=l2, tol=0.001;, score=0.819 total time=   0.8s\n",
      "[CV 1/5; 45/48] START C=10, max_iter=200, penalty=l2, tol=0.01..................\n",
      "[CV 1/5; 45/48] END C=10, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 2/5; 45/48] START C=10, max_iter=200, penalty=l2, tol=0.01..................\n",
      "[CV 2/5; 45/48] END C=10, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 3/5; 45/48] START C=10, max_iter=200, penalty=l2, tol=0.01..................\n",
      "[CV 3/5; 45/48] END C=10, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.5s\n",
      "[CV 4/5; 45/48] START C=10, max_iter=200, penalty=l2, tol=0.01..................\n",
      "[CV 4/5; 45/48] END C=10, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 5/5; 45/48] START C=10, max_iter=200, penalty=l2, tol=0.01..................\n",
      "[CV 5/5; 45/48] END C=10, max_iter=200, penalty=l2, tol=0.01;, score=0.498 total time=   0.2s\n",
      "[CV 1/5; 46/48] START C=10, max_iter=200, penalty=l1, tol=0.0001................\n",
      "[CV 1/5; 46/48] END C=10, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 46/48] START C=10, max_iter=200, penalty=l1, tol=0.0001................\n",
      "[CV 2/5; 46/48] END C=10, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 46/48] START C=10, max_iter=200, penalty=l1, tol=0.0001................\n",
      "[CV 3/5; 46/48] END C=10, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 46/48] START C=10, max_iter=200, penalty=l1, tol=0.0001................\n",
      "[CV 4/5; 46/48] END C=10, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 46/48] START C=10, max_iter=200, penalty=l1, tol=0.0001................\n",
      "[CV 5/5; 46/48] END C=10, max_iter=200, penalty=l1, tol=0.0001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 47/48] START C=10, max_iter=200, penalty=l1, tol=0.001.................\n",
      "[CV 1/5; 47/48] END C=10, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 2/5; 47/48] START C=10, max_iter=200, penalty=l1, tol=0.001.................\n",
      "[CV 2/5; 47/48] END C=10, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 3/5; 47/48] START C=10, max_iter=200, penalty=l1, tol=0.001.................\n",
      "[CV 3/5; 47/48] END C=10, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 4/5; 47/48] START C=10, max_iter=200, penalty=l1, tol=0.001.................\n",
      "[CV 4/5; 47/48] END C=10, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 5/5; 47/48] START C=10, max_iter=200, penalty=l1, tol=0.001.................\n",
      "[CV 5/5; 47/48] END C=10, max_iter=200, penalty=l1, tol=0.001;, score=nan total time=   0.0s\n",
      "[CV 1/5; 48/48] START C=10, max_iter=200, penalty=l1, tol=0.01..................\n",
      "[CV 1/5; 48/48] END C=10, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 2/5; 48/48] START C=10, max_iter=200, penalty=l1, tol=0.01..................\n",
      "[CV 2/5; 48/48] END C=10, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 3/5; 48/48] START C=10, max_iter=200, penalty=l1, tol=0.01..................\n",
      "[CV 3/5; 48/48] END C=10, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 4/5; 48/48] START C=10, max_iter=200, penalty=l1, tol=0.01..................\n",
      "[CV 4/5; 48/48] END C=10, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n",
      "[CV 5/5; 48/48] START C=10, max_iter=200, penalty=l1, tol=0.01..................\n",
      "[CV 5/5; 48/48] END C=10, max_iter=200, penalty=l1, tol=0.01;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\ML projects\\sentiment_analysis\\env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "120 fits failed out of a total of 240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "120 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\ML projects\\sentiment_analysis\\env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"g:\\ML projects\\sentiment_analysis\\env\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\ML projects\\sentiment_analysis\\env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"g:\\ML projects\\sentiment_analysis\\env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "g:\\ML projects\\sentiment_analysis\\env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [0.80713095 0.80609524 0.49809524        nan        nan        nan\n",
      " 0.80713095 0.80609524 0.49809524        nan        nan        nan\n",
      " 0.81817857 0.81808333 0.49809524        nan        nan        nan\n",
      " 0.81817857 0.81808333 0.49809524        nan        nan        nan\n",
      " 0.8187381  0.81930952 0.49809524        nan        nan        nan\n",
      " 0.8187381  0.81930952 0.49809524        nan        nan        nan\n",
      " 0.81865476 0.81894048 0.49809524        nan        nan        nan\n",
      " 0.81865476 0.81894048 0.49809524        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score {np.float64(0.8193095238095237)}\n",
      "Best hyperparameter set:\n",
      "\tC: 1.0\n",
      "\tmax_iter: 100\n",
      "\tpenalty: l2\n",
      "\ttol: 0.001\n"
     ]
    }
   ],
   "source": [
    "param_gd = {\"penalty\" : [\"l2\", \"l1\"],\n",
    "         \"C\" : [0.01, 0.1, 1.0, 10],\n",
    "         \"tol\" : [0.0001, 0.001, 0.01],\n",
    "         \"max_iter\" : [100, 200]}\n",
    "\n",
    "model_lgr_optimized, best_param = hyperparamtune(LogisticRegression(), param_gd, \"accuracy\", 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of FineTuned Logsitic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score on training dateset for Finetuned Logsitic Regression Classifier: 0.8225595238095238\n",
      "AUC Score on training dateset for Finetuned Logsitic Regression Classifier: 0.9034188696275864\n",
      "F1 Score training dateset for Finetuned Logsitic Regression Classifier: 0.8225495091323806\n",
      "\n",
      "\n",
      "Precision Score on test for Finetuned Logsitic Regression Classifier: 0.8158055555555556\n",
      "AUC Score on test for Finetuned Logsitic Regression Classifier: 0.8984561249283894\n",
      "F1 Score for Finetuned Logsitic Regression Classifier: 0.8158079112907944\n"
     ]
    }
   ],
   "source": [
    "train_prediction = model_lgr_optimized.predict(x_train_tfidf)\n",
    "\n",
    "print(\"Precision Score on training dateset for Finetuned Logsitic Regression Classifier: %s\" % precision_score(y_train, train_prediction, average='micro'))\n",
    "print(\"AUC Score on training dateset for Finetuned Logsitic Regression Classifier: %s\" % roc_auc_score(y_train, model_lgr_optimized.predict_proba(x_train_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_train_6 = f1_score(y_train, train_prediction, average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Finetuned Logsitic Regression Classifier: %s\" % f1_score_train_6)\n",
    "print(\"\\n\")\n",
    "\n",
    "test_prediction = model_lgr_optimized.predict(x_test_tfidf)\n",
    "\n",
    "print(\"Precision Score on test for Finetuned Logsitic Regression Classifier: %s\" % precision_score(y_test, test_prediction, average='micro'))\n",
    "print(\"AUC Score on test for Finetuned Logsitic Regression Classifier: %s\" % roc_auc_score(y_test, model_lgr_optimized.predict_proba(x_test_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_6 = f1_score(y_test, test_prediction, average=\"weighted\")\n",
    "print(\"F1 Score for Finetuned Logsitic Regression Classifier: %s\" % f1_score_6)\n",
    "\n",
    "model_to_f1_score[model_lgr_optimized] = f1_score_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is Pipeline(steps=[('classifier', LogisticRegression())]) with an F1 score of 0.8163628646101859.\n"
     ]
    }
   ],
   "source": [
    "best_model = max(model_to_f1_score, key=model_to_f1_score.get)\n",
    "best_f1_score = model_to_f1_score[best_model]\n",
    "\n",
    "print(f\"The best model is {best_model} with an F1 score of {best_f1_score}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_gd = {\"n_estimators\" : [100, 200, 300],\n",
    "         \"max_depth\" : [11, 13, 17, 19, 23],\n",
    "         \"criterion\" : [\"gini\", \"entropy\"],\n",
    "         \"min_samples_split\" : [3, 7, 11],\n",
    "         \"min_samples_leaf\" : [3, 5],\n",
    "         \"max_features\" : [\"sqrt\", \"log2\"]}\n",
    "\n",
    "model_rfc_optimized, best_param = hyperparamtune(RandomForestClassifier(), param_gd, \"accuracy\", 10, 5)\n",
    "\n",
    "## Taking too much time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Finetuned Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = model_rfc_optimized.predict(x_train_tfidf)\n",
    "\n",
    "print(\"Precision Score on training dateset for Finetuned Random Forest Classifier: %s\" % precision_score(y_train, train_prediction, average='micro'))\n",
    "print(\"AUC Score on training dateset for Finetuned Random Forest Classifier: %s\" % roc_auc_score(y_train, model_rfc_optimized.predict_proba(x_train_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_train_8 = f1_score(y_train, train_prediction, average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Finetuned Random Forest Classifier: %s\" % f1_score_train_8)\n",
    "print(\"\\n\")\n",
    "\n",
    "test_prediction = model_rfc_optimized.predict(x_test_tfidf)\n",
    "\n",
    "print(\"Precision Score on test for Finetuned Random Forest Classifier: %s\" % precision_score(y_test, test_prediction, average='micro'))\n",
    "print(\"AUC Score on test for Finetuned Random Forest Classifier: %s\" % roc_auc_score(y_test, model_rfc_optimized.predict_proba(x_test_tfidf)[:,1], multi_class='ovo', average='macro'))\n",
    "\n",
    "f1_score_8 = f1_score(y_test, test_prediction, average=\"weighted\")\n",
    "print(\"F1 Score for Finetuned Random Forest Classifier: %s\" % f1_score_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning for Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_gd = {\"base_estimator\" : [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=4), DecisionTreeClassifier(max_depth=8), DecisionTreeClassifier(max_depth=10)],\n",
    "          \"learning_rate\" : [0.001, 0.01, 0.1, 0.5, 0.8, 1, 2],\n",
    "          \"n_estimators\":[50, 100, 200, 300, 500, 800]}\n",
    "\n",
    "model_10, best_param_10 = hyperparamtune(AdaBoostClassifier(), param_gd, \"accuracy\", 10, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
